{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b723c379-7818-41ff-823a-9cd510efa286",
    "_uuid": "e9243f53-cb13-4974-88ba-6db3d95e7e22",
    "execution": {
     "iopub.execute_input": "2021-07-28T10:38:33.989060Z",
     "iopub.status.busy": "2021-07-28T10:38:33.988742Z",
     "iopub.status.idle": "2021-07-28T10:38:33.998804Z",
     "shell.execute_reply": "2021-07-28T10:38:33.997957Z",
     "shell.execute_reply.started": "2021-07-28T10:38:33.989031Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "## define custom magic to save most useful classes and use them in inference notebook \n",
    "## instead of copying the code every time you have changes in the classes\n",
    "@register_cell_magic\n",
    "def write_and_run(line, cell):\n",
    "    argz = line.split()\n",
    "    file = argz[-1]\n",
    "    mode = 'w'\n",
    "    if len(argz) == 2 and argz[0] == '-a':\n",
    "        mode = 'a'\n",
    "    with open(file, mode) as f:\n",
    "        f.write(cell)\n",
    "    get_ipython().run_cell(cell)\n",
    "    \n",
    "Path('/home/fcq/Li/nlp_learn/kaggle/scripts').mkdir(exist_ok=True)\n",
    "models_dir = Path('/home/fcq/Li/nlp_learn/kaggle/working/models')\n",
    "models_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "4c1cc923-4884-4ee6-87f1-0759b90f391b",
    "_uuid": "cbbfbd34-f381-4923-b6dc-2c0731e75e22",
    "execution": {
     "iopub.execute_input": "2021-07-28T10:38:34.001105Z",
     "iopub.status.busy": "2021-07-28T10:38:34.000436Z",
     "iopub.status.idle": "2021-07-28T10:38:41.759910Z",
     "shell.execute_reply": "2021-07-28T10:38:41.759082Z",
     "shell.execute_reply.started": "2021-07-28T10:38:34.001035Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%write_and_run scripts/imports.py\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler, Sampler\n",
    "from torch.nn.functional import mse_loss\n",
    "from transformers import AutoModel,AutoTokenizer,get_cosine_schedule_with_warmup, AutoConfig, AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-talk')\n",
    "# print(plt.style.available)\n",
    "from time import time\n",
    "from colorama import Fore, Back, Style\n",
    "r_ = Fore.RED\n",
    "b_ = Fore.BLUE\n",
    "g_ = Fore.GREEN\n",
    "y_ = Fore.YELLOW\n",
    "w_ = Fore.WHITE\n",
    "bb_ = Back.BLACK\n",
    "sr_ = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "3769f1c1-a20e-47ff-a14a-78f928c35c45",
    "_uuid": "e3847e24-fa00-4f20-be41-e18a9cad8f6d",
    "execution": {
     "iopub.execute_input": "2021-07-28T10:38:41.763632Z",
     "iopub.status.busy": "2021-07-28T10:38:41.763340Z",
     "iopub.status.idle": "2021-07-28T10:38:41.769857Z",
     "shell.execute_reply": "2021-07-28T10:38:41.768955Z",
     "shell.execute_reply.started": "2021-07-28T10:38:41.763606Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%write_and_run scripts/config.py\n",
    "\n",
    "class Config:\n",
    "    model_name = 'roberta-large'\n",
    "    output_hidden_states = True\n",
    "    epochs = 2\n",
    "#     evaluate_interval = 40\n",
    "    batch_size = 8\n",
    "    device = 'cuda'\n",
    "    seed = 42\n",
    "    max_len = 256\n",
    "    lr = 1e-5\n",
    "    wd = 0.01\n",
    "#     eval_schedule = [(float('inf'), 40), (0.5, 30), (0.49, 20), (0.48, 10), (0.47, 3), (0, 0)]\n",
    "    eval_schedule = [(float('inf'), 40), (0.47, 20), (0.46, 10), (0, 0)]\n",
    "\n",
    "    gradient_accumulation = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "978ff8b8-aef0-4355-9f3d-a4e26f3d133e",
    "_uuid": "f50233e4-91e5-4938-9021-cd3527ffb92b",
    "execution": {
     "iopub.execute_input": "2021-07-28T10:38:41.771873Z",
     "iopub.status.busy": "2021-07-28T10:38:41.771496Z",
     "iopub.status.idle": "2021-07-28T10:38:41.783490Z",
     "shell.execute_reply": "2021-07-28T10:38:41.782676Z",
     "shell.execute_reply.started": "2021-07-28T10:38:41.771835Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONASSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(seed=Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "dda3bd25-151c-4db7-8c42-8b0d82418000",
    "_uuid": "f1a44a6b-a9cc-4b7d-ad1a-beb940280ccb",
    "execution": {
     "iopub.execute_input": "2021-07-28T10:38:41.785251Z",
     "iopub.status.busy": "2021-07-28T10:38:41.784900Z",
     "iopub.status.idle": "2021-07-28T10:38:42.013649Z",
     "shell.execute_reply": "2021-07-28T10:38:42.012826Z",
     "shell.execute_reply.started": "2021-07-28T10:38:41.785216Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('../input/k/chamecall/train-val-split/train.csv')\n",
    "# val_df = pd.read_csv('../input/k/chamecall/train-val-split/val.csv')\n",
    " \n",
    "kfold_df = pd.read_csv('/home/fcq/Li/nlp_learn/datasets/kaggle_CLPR/new_kfold.csv')\n",
    "aux_kfold_df = pd.read_csv('/home/fcq/Li/nlp_learn/datasets/kaggle_CLPR/kfold_parsed_annotated_by_all_models.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "7ed2c1e9-03c8-4e29-805f-0c5f9913ae9a",
    "_uuid": "fd475a2f-40ce-423e-aa4b-dca10e5dba41",
    "execution": {
     "iopub.execute_input": "2021-07-28T10:38:42.015210Z",
     "iopub.status.busy": "2021-07-28T10:38:42.014871Z",
     "iopub.status.idle": "2021-07-28T10:38:42.027291Z",
     "shell.execute_reply": "2021-07-28T10:38:42.026307Z",
     "shell.execute_reply.started": "2021-07-28T10:38:42.015175Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%write_and_run scripts/dataset.py\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "def convert_examples_to_features(text, tokenizer, max_len):\n",
    "\n",
    "    tok = tokenizer.encode_plus(\n",
    "        text, \n",
    "        max_length=max_len, \n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    return tok\n",
    "\n",
    "\n",
    "class CLRPDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        self.data = data\n",
    "        self.excerpts = self.data.excerpt.tolist()\n",
    "        if not is_test:\n",
    "            self.targets = self.data.target.tolist()\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if not self.is_test:\n",
    "            excerpt = self.excerpts[item]\n",
    "            label = self.targets[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, self.max_len\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "                'label':torch.tensor(label, dtype=torch.float),\n",
    "            }\n",
    "        else:\n",
    "            excerpt = self.excerpts[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, self.max_len\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "802baa90-c3ce-4db2-818e-c7fec6bfa86a",
    "_uuid": "e79914ba-fe43-4a59-b7b9-43e9ca9f1762",
    "execution": {
     "iopub.execute_input": "2021-07-28T10:38:42.030436Z",
     "iopub.status.busy": "2021-07-28T10:38:42.030105Z",
     "iopub.status.idle": "2021-07-28T10:38:42.042526Z",
     "shell.execute_reply": "2021-07-28T10:38:42.041533Z",
     "shell.execute_reply.started": "2021-07-28T10:38:42.030399Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%write_and_run scripts/model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, h_size, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(h_size, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.W(features))\n",
    "        score = self.V(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "class CLRPModel(nn.Module):\n",
    "    def __init__(self,transformer,config):\n",
    "        super(CLRPModel,self).__init__()\n",
    "        self.h_size = config.hidden_size\n",
    "        self.transformer = transformer\n",
    "        self.head = AttentionHead(self.h_size*4)\n",
    "        self.linear = nn.Linear(self.h_size*2, 1)\n",
    "        self.linear_out = nn.Linear(self.h_size*8, 1)\n",
    "\n",
    "              \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        transformer_out = self.transformer(input_ids, attention_mask)\n",
    "       \n",
    "        all_hidden_states = torch.stack(transformer_out.hidden_states)\n",
    "        cat_over_last_layers = torch.cat(\n",
    "            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n",
    "        )\n",
    "        \n",
    "        cls_pooling = cat_over_last_layers[:, 0]   \n",
    "        head_logits = self.head(cat_over_last_layers)\n",
    "        y_hat = self.linear_out(torch.cat([head_logits, cls_pooling], -1))\n",
    "        \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "43fa2135-66a5-4602-850b-95998012274b",
    "_uuid": "675f9668-1449-48b3-917e-e46662913156",
    "execution": {
     "iopub.execute_input": "2021-07-28T10:38:42.047283Z",
     "iopub.status.busy": "2021-07-28T10:38:42.046964Z",
     "iopub.status.idle": "2021-07-28T10:38:42.058970Z",
     "shell.execute_reply": "2021-07-28T10:38:42.057944Z",
     "shell.execute_reply.started": "2021-07-28T10:38:42.047254Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_optimizer(model):\n",
    "    named_parameters = list(model.named_parameters())    \n",
    "    \n",
    "    roberta_parameters = named_parameters[:389]    \n",
    "    attention_parameters = named_parameters[391:395]\n",
    "    regressor_parameters = named_parameters[395:]\n",
    "        \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "    \n",
    "    parameters = []\n",
    "    parameters.append({\"params\": attention_group})\n",
    "    parameters.append({\"params\": regressor_group})\n",
    "    # increase lr every second layer\n",
    "    increase_lr_every_k_layer = 1\n",
    "    lrs = np.linspace(1, 5, 24 // increase_lr_every_k_layer)\n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "        splitted_name = name.split('.')\n",
    "        lr = Config.lr\n",
    "        if len(splitted_name) >= 4 and str.isdigit(splitted_name[3]):\n",
    "            layer_num = int(splitted_name[3])\n",
    "            lr = lrs[layer_num // increase_lr_every_k_layer] * Config.lr \n",
    "\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "\n",
    "    return optim.AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "aae88cc3-fb02-438a-b9da-beb9055f38fc",
    "_uuid": "d8f5e59f-9098-47e6-aecb-ce97cda6f90c",
    "execution": {
     "iopub.execute_input": "2021-07-28T10:38:42.102148Z",
     "iopub.status.busy": "2021-07-28T10:38:42.101651Z",
     "iopub.status.idle": "2021-07-28T13:46:54.157495Z",
     "shell.execute_reply": "2021-07-28T13:46:54.156511Z",
     "shell.execute_reply.started": "2021-07-28T10:38:42.102071Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[40m\u001b[37m  Model#1  \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4007ef81f56549e58e46aaf1424cc3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3950/2682084404.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#     val_dl = make_dataloader(val_df, tokenizer, is_train=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLRPModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/plm/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         raise ValueError(\n\u001b[1;32m    386\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/plm/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m                 \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                 resolved_archive_file = cached_path(\n\u001b[0m\u001b[1;32m   1250\u001b[0m                     \u001b[0marchive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m                     \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/plm/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m   1364\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/plm/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1624\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{url} not found in cache or force_download set to True, downloading to {temp_file.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m             \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"storing {url} in cache at {cache_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/plm/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m     )\n\u001b[0;32m-> 1485\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/plm/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/plm/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/plm/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m                 if (\n\u001b[1;32m    521\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/plm/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/plm/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/plm/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/plm/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/plm/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_scores = []\n",
    "\n",
    "for model_num in range(5): \n",
    "    print(f'{bb_}{w_}  Model#{model_num+1}  {sr_}')\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n",
    "    config = AutoConfig.from_pretrained(Config.model_name)\n",
    "    config.update({\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7,\n",
    "            \"output_hidden_states\": True\n",
    "            }) \n",
    "    \n",
    "    l_train_fold = kfold_df[kfold_df.fold!=model_num]\n",
    "    r_train_fold = aux_kfold_df[aux_kfold_df.fold!=model_num]\n",
    "    train_df = pd.concat([l_train_fold, r_train_fold])\n",
    "    \n",
    "    train_dl = make_dataloader(train_df, tokenizer)\n",
    "    val_dl = make_dataloader(kfold_df[kfold_df.fold==model_num], tokenizer, is_train=False)\n",
    "\n",
    "#     train_dl = make_dataloader(train_df, tokenizer)\n",
    "#     val_dl = make_dataloader(val_df, tokenizer, is_train=False)\n",
    "\n",
    "    transformer = AutoModel.from_pretrained(Config.model_name, config=config)  \n",
    "\n",
    "    model = CLRPModel(transformer, config)\n",
    "    \n",
    "    model = model.to(Config.device)\n",
    "    optimizer = create_optimizer(model)\n",
    "    scaler = GradScaler()\n",
    "#     optimizer = optim.AdamW(model.parameters(), lr=Config.lr)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_training_steps=Config.epochs * len(train_dl),\n",
    "            num_warmup_steps=len(train_dl) * Config.epochs * 0.11)  \n",
    "\n",
    "    criterion = mse_loss\n",
    "\n",
    "    trainer = Trainer(train_dl, val_dl, model, optimizer, scheduler, scaler, criterion, model_num)\n",
    "    record_info, best_val_loss = trainer.run()\n",
    "    best_scores.append(best_val_loss)    \n",
    "    \n",
    "    steps, train_losses = list(zip(*record_info['train_loss']))\n",
    "    plt.plot(steps, train_losses, label='train_loss')\n",
    "    steps, val_losses = list(zip(*record_info['val_loss']))\n",
    "    plt.plot(steps, val_losses, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "print('Best val losses:', best_scores)\n",
    "print('Avg val loss:', np.array(best_scores).mean())\n",
    "!date '+%A %W %Y %X' > execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aed8bccb-d845-46e4-a168-f661277bc804",
    "_uuid": "2c1feb5d-35aa-42f0-8906-90f6924b57af",
    "execution": {
     "iopub.execute_input": "2021-07-28T10:38:42.063129Z",
     "iopub.status.busy": "2021-07-28T10:38:42.062844Z",
     "iopub.status.idle": "2021-07-28T10:38:42.100136Z",
     "shell.execute_reply": "2021-07-28T10:38:42.099182Z",
     "shell.execute_reply.started": "2021-07-28T10:38:42.063102Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "   \n",
    "    \n",
    "class DynamicPadCollate:\n",
    "    def __call__(self,batch):\n",
    "                \n",
    "        out = {'input_ids' :[],\n",
    "               'attention_mask':[],\n",
    "                'label':[]\n",
    "        }\n",
    "        \n",
    "        for i in batch:\n",
    "            for k,v in i.items():\n",
    "                out[k].append(v)\n",
    "                \n",
    "        max_pad =0\n",
    "\n",
    "        for p in out['input_ids']:\n",
    "            if max_pad < len(p):\n",
    "                max_pad = len(p)\n",
    "                    \n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            \n",
    "            input_id = out['input_ids'][i]\n",
    "            att_mask = out['attention_mask'][i]\n",
    "            text_len = len(input_id)\n",
    "            \n",
    "            out['input_ids'][i] = (out['input_ids'][i].tolist() + [1] * (max_pad - text_len))[:max_pad]\n",
    "            out['attention_mask'][i] = (out['attention_mask'][i].tolist() + [0] * (max_pad - text_len))[:max_pad]\n",
    "        \n",
    "        out['input_ids'] = torch.tensor(out['input_ids'],dtype=torch.long)\n",
    "        out['attention_mask'] = torch.tensor(out['attention_mask'],dtype=torch.long)\n",
    "        out['label'] = torch.tensor(out['label'],dtype=torch.float)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class AvgCounter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def update(self, loss, n_samples):\n",
    "        self.loss += loss * n_samples\n",
    "        self.n_samples += n_samples\n",
    "        \n",
    "    def avg(self):\n",
    "        return self.loss / self.n_samples\n",
    "    \n",
    "    def reset(self):\n",
    "        self.loss = 0\n",
    "        self.n_samples = 0\n",
    "\n",
    "class EvaluationScheduler:\n",
    "    def __init__(self, evaluation_schedule, penalize_factor=1, max_penalty=8):\n",
    "        self.evaluation_schedule = evaluation_schedule\n",
    "        self.evaluation_interval = self.evaluation_schedule[0][1]\n",
    "        self.last_evaluation_step = 0\n",
    "        self.prev_loss = float('inf')\n",
    "        self.penalize_factor = penalize_factor\n",
    "        self.penalty = 0\n",
    "        self.prev_interval = -1\n",
    "        self.max_penalty = max_penalty\n",
    "\n",
    "    def step(self, step):\n",
    "        # should we to make evaluation right now\n",
    "        if step >= self.last_evaluation_step + self.evaluation_interval:\n",
    "            self.last_evaluation_step = step\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "            \n",
    "    def update_evaluation_interval(self, last_loss):\n",
    "        # set up evaluation_interval depending on loss value\n",
    "        cur_interval = -1\n",
    "        for i, (loss, interval) in enumerate(self.evaluation_schedule[:-1]):\n",
    "            if self.evaluation_schedule[i+1][0] < last_loss < loss:\n",
    "                self.evaluation_interval = interval\n",
    "                cur_interval = i\n",
    "                break\n",
    "#         if last_loss > self.prev_loss and self.prev_interval == cur_interval:\n",
    "#             self.penalty += self.penalize_factor\n",
    "#             self.penalty = min(self.penalty, self.max_penalty)\n",
    "#             self.evaluation_interval += self.penalty\n",
    "#         else:\n",
    "#             self.penalty = 0\n",
    "            \n",
    "        self.prev_loss = last_loss\n",
    "        self.prev_interval = cur_interval\n",
    "        \n",
    "          \n",
    "        \n",
    "def make_dataloader(data, tokenizer, is_train=True):\n",
    "    dataset = CLRPDataset(data, tokenizer=tokenizer, max_len=Config.max_len)\n",
    "    if is_train:\n",
    "        sampler = RandomSampler(dataset)\n",
    "    else:\n",
    "        sampler = SequentialSampler(dataset)\n",
    "\n",
    "    batch_dataloader = DataLoader(dataset, sampler=sampler, batch_size=Config.batch_size, pin_memory=True, collate_fn=DynamicPadCollate())\n",
    "    return batch_dataloader\n",
    "                   \n",
    "            \n",
    "class Trainer:\n",
    "    def __init__(self, train_dl, val_dl, model, optimizer, scheduler, scaler, criterion, model_num):\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = Config.device\n",
    "        self.batches_per_epoch = len(self.train_dl)\n",
    "        self.total_batch_steps = self.batches_per_epoch * Config.epochs\n",
    "        self.criterion = criterion\n",
    "        self.model_num = model_num\n",
    "        \n",
    "        self.scaler = scaler\n",
    "                \n",
    "    def run(self):\n",
    "        patience = 15\n",
    "        record_info = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "        }\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        evaluation_scheduler = EvaluationScheduler(Config.eval_schedule)\n",
    "        train_loss_counter = AvgCounter()\n",
    "        step = 0\n",
    "        \n",
    "        model_updated_step = 0\n",
    "        evaluation_step = 0\n",
    "        \n",
    "        for epoch in range(Config.epochs):\n",
    "            \n",
    "            print(f'{r_}Epoch: {epoch+1}/{Config.epochs}{sr_}')\n",
    "            start_epoch_time = time()\n",
    "            \n",
    "            for batch_num, batch in enumerate(self.train_dl):\n",
    "                train_loss = self.train(batch, step)\n",
    "#                 print(f'{epoch+1}#[{step+1}/{len(self.train_dl)}]: train loss - {train_loss.item()}')\n",
    "\n",
    "                train_loss_counter.update(train_loss, len(batch))\n",
    "                record_info['train_loss'].append((step, train_loss.item()))\n",
    "\n",
    "                if evaluation_scheduler.step(step):\n",
    "                    val_loss = self.evaluate()\n",
    "                    \n",
    "                    record_info['val_loss'].append((step, val_loss.item()))        \n",
    "                    print(f'\\t\\t{bb_}{r_}[{evaluation_step-model_updated_step}] {sr_}{epoch+1}#[{batch_num+1}/{self.batches_per_epoch}]: train loss - {train_loss_counter.avg()} | val loss - {val_loss}',)\n",
    "                    train_loss_counter.reset()\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        \n",
    "                        best_val_loss = val_loss.item()\n",
    "                        print(f\"\\t\\t{g_}Val loss decreased from {best_val_loss} to {val_loss}{sr_}\")\n",
    "                        torch.save(self.model, models_dir / f'best_model_{self.model_num}.pt')\n",
    "                        model_updated_step = evaluation_step\n",
    "                        \n",
    "                        \n",
    "                    evaluation_scheduler.update_evaluation_interval(val_loss.item())\n",
    "                    evaluation_step += 1\n",
    "                    if evaluation_step - model_updated_step > patience:\n",
    "                        print(f'\\t{bb_}{r_}Model does\"t converge. Stop training...{sr_}')\n",
    "                        break         \n",
    "                \n",
    "                step += 1\n",
    "\n",
    "                \n",
    "            end_epoch_time = time()\n",
    "            print(f'{bb_}{y_}The epoch took {end_epoch_time - start_epoch_time} sec..{sr_}')\n",
    "\n",
    "        return record_info, best_val_loss\n",
    "            \n",
    "\n",
    "    def train(self, batch, batch_step):\n",
    "        self.model.train()\n",
    "        sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device)\n",
    "        with autocast():\n",
    "            preds = self.model(sent_id, mask)\n",
    "            train_loss = self.criterion(preds, labels.unsqueeze(1))\n",
    "        \n",
    "        self.scaler.scale(train_loss).backward()\n",
    "#         train_loss.backward()\n",
    "        \n",
    "        if (batch_step + 1) % Config.gradient_accumulation or batch_step+1 == self.total_batch_steps:\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "#             self.optimizer.step()\n",
    "            self.model.zero_grad() \n",
    "        self.scheduler.step()\n",
    "        return torch.sqrt(train_loss)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        val_loss_counter = AvgCounter()\n",
    "\n",
    "        for step,batch in enumerate(self.val_dl):\n",
    "            sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device)\n",
    "            with torch.no_grad():\n",
    "                with autocast():\n",
    "                    preds = self.model(sent_id, mask)\n",
    "                    loss = self.criterion(preds,labels.unsqueeze(1))\n",
    "                val_loss_counter.update(torch.sqrt(loss), len(labels))\n",
    "        return val_loss_counter.avg()\n",
    "    \n",
    "    \n",
    "def mse_loss(y_true,y_pred):\n",
    "    return nn.functional.mse_loss(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
